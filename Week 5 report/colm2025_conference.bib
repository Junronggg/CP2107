@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017},
  publisher={Curran Associates, Inc.}
}

@article{zhang2024transformer_explain,
  author={Zhang, Jinpeng},
  title={Transformer Clear Explanation: “Attention is All You Need”! — 2017},
  journal={Medium},
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2018},
  note={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2021multihead_vs_single,
  title={Multi-head or Single-head? An Empirical Comparison for Transformer Training},
  author={Liu, Liyuan and Liu, Jialu and Han, Jiawei},
  journal={arXiv preprint arXiv:2106.09650},
  year={2021}
}

@article{chafiqui2024deepdive_transformer,
  author={Chafiqui, Youssef},
  title={Part 3: A Comprehensive Deep Dive into the Theory and Mechanisms of Transformer-Based Neural Networks},
  journal={Medium},
  year={2024}
}


