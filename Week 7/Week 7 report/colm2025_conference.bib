@misc{qwen2.5-0.5b,
  title        = {Qwen2.5-0.5B},
  author       = {Qwen Team, Alibaba DAMO NLP},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen2.5-0.5B}},
  note         = {Accessed: 2025-06-27}
}

@misc{qwen2.5-0.5b-instruct,
  title        = {Qwen2.5-0.5B-Instruct},
  author       = {Qwen Team, Alibaba DAMO NLP},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct}},
  note         = {Accessed: 2025-06-27}
}

@misc{qwen3-0.6b-base,
  title        = {Qwen3-0.6B-Base},
  author       = {Qwen Team, Alibaba DAMO NLP},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen3-0.6B-Base}},
  note         = {Accessed: 2025-06-27}
}

@misc{qwen3-0.6b,
  title        = {Qwen3-0.6B (Instruct)},
  author       = {Qwen Team, Alibaba DAMO NLP},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen3-0.6B}},
  note         = {Accessed: 2025-06-27}
}

@article{vllm2023,
  title     = {vLLM: Easy, Fast, and Cheap LLM Inference with PagedAttention},
  author    = {Zheng, Zihao and Chiang, Wei-Lin and Li, Shouqian and Lin, Kevin and others},
  journal   = {arXiv preprint arXiv:2309.06180},
  year      = {2023},
  url       = {https://arxiv.org/abs/2309.06180}
}

@inproceedings{holtzman2019curious,
  title     = {The Curious Case of Neural Text Degeneration},
  author    = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020},
  url       = {https://arxiv.org/abs/1904.09751}
}

@article{ouyang2022training,
  title     = {Training language models to follow instructions with human feedback},
  author    = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and others},
  journal   = {arXiv preprint arXiv:2203.02155},
  year      = {2022},
  url       = {https://arxiv.org/abs/2203.02155}
}

@article{dao2022flashattention,
  title     = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author    = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and others},
  journal   = {arXiv preprint arXiv:2205.14135},
  year      = {2022},
  url       = {https://arxiv.org/abs/2205.14135}
}
