@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  publisher={OpenAI}
}

@article{radford2019language,
  title={Language Models are Few-Shot Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  publisher={Curran Associates, Inc.}
}

@misc{nanoGPT,
  author = {Karpathy, Andrej},
  title = {nanoGPT},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
  year = {2022},
  note = {The simplest, fastest repository for training/finetuning medium-sized GPTs}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@book{goodfellow2016deep,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher={MIT Press},
  year={2016},
  chapter={5.5},
  note={Cross-entropy loss derivation used in nanoGPT}
}

@misc{nanoGPT-vs-Qwen,
  author = {Karpathy, Andrej},
  title = {Tweet comparing model approaches},
  year = {2023},
  howpublished = {\url{https://twitter.com/karpathy/status/1645115623017541633}},
  note = {Key differences in design philosophy}
}